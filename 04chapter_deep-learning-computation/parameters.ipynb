{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型参数的访问、初始化和共享\n",
    "\n",
    "在[“线性回归的简洁实现”](../chapter_deep-learning-basics/linear-regression-gluon.ipynb)一节中，我们通过`init`模块来初始化模型的全部参数。我们也介绍了访问模型参数的简单方法。本节将深入讲解如何访问和初始化模型参数，以及如何在多个层之间共享同一份模型参数。\n",
    "\n",
    "我们先定义一个与上一节中相同的含单隐藏层的多层感知机。我们依然使用默认方式初始化它的参数，并做一次前向计算。与之前不同的是，在这里我们从MXNet中导入了`init`模块，它包含了多种模型初始化方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.03889871 -0.0003221   0.04652183 -0.02139459 -0.0506786   0.02504038\n",
       "  -0.07105569  0.0348109   0.00928678 -0.02688462]\n",
       " [ 0.03837157  0.01684525  0.0444224  -0.01695633 -0.05066484  0.03306553\n",
       "  -0.09300549  0.04911706  0.00795607 -0.03339119]]\n",
       "<NDArray 2x10 @cpu(0)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mxnet import init, nd\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, activation='relu'))\n",
    "net.add(nn.Dense(10))\n",
    "net.initialize()  # 使用默认初始化方式\n",
    "\n",
    "X = nd.random.uniform(shape=(2, 20))\n",
    "Y = net(X)  # 前向计算\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 访问模型参数\n",
    "\n",
    "对于使用`Sequential`类构造的神经网络，我们可以通过方括号`[]`来访问网络的任一层。回忆一下上一节中提到的`Sequential`类与`Block`类的继承关系。对于`Sequential`实例中含模型参数的层，我们可以通过`Block`类的`params`属性来访问该层包含的所有参数。下面，访问多层感知机`net`中隐藏层的所有参数。索引0表示隐藏层为`Sequential`实例最先添加的层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dense12_ (\n",
       "   Parameter dense12_weight (shape=(256, 20), dtype=float32)\n",
       "   Parameter dense12_bias (shape=(256,), dtype=float32)\n",
       " ), mxnet.gluon.parameter.ParameterDict)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].params, type(net[0].params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，我们得到了一个由参数名称映射到参数实例的字典（类型为`ParameterDict`类）。其中权重参数的名称为`dense0_weight`，它由`net[0]`的名称（`dense0_`）和自己的变量名（`weight`）组成。而且可以看到，该参数的形状为(256, 20)，且数据类型为32位浮点数（`float32`）。为了访问特定参数，我们既可以通过名字来访问字典里的元素，也可以直接使用它的变量名。下面两种方法是等价的，但通常后者的代码可读性更好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dense0_weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-a78ecb1f6642>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dense0_weight'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\anaconda201903\\envs\\gluon\\lib\\site-packages\\mxnet\\gluon\\parameter.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    655\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    656\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 657\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    659\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'dense0_weight'"
     ]
    }
   ],
   "source": [
    "net[0].params['dense0_weight'], net[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gluon里参数类型为`Parameter`类，它包含参数和梯度的数值，可以分别通过`data`函数和`grad`函数来访问。因为我们随机初始化了权重，所以权重参数是一个由随机数组成的形状为(256, 20)的`NDArray`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.06435739 -0.03611815 -0.03635908 ... -0.00210812 -0.01162925\n",
       "   0.0393104 ]\n",
       " [-0.01393424  0.02078956  0.0273485  ...  0.04024323  0.04453081\n",
       "  -0.00120679]\n",
       " [ 0.01600849  0.01238615 -0.03899138 ... -0.02004729 -0.02252676\n",
       "  -0.03604552]\n",
       " ...\n",
       " [-0.04613923  0.01271766  0.01243475 ... -0.06454763  0.03158209\n",
       "  -0.00245084]\n",
       " [-0.05414722  0.01907761 -0.02411117 ...  0.034913   -0.05175916\n",
       "   0.04666357]\n",
       " [ 0.02206432  0.02771549  0.03542814 ... -0.02843853 -0.00084343\n",
       "   0.05313788]]\n",
       "<NDArray 256x20 @cpu(0)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "权重梯度的形状和权重的形状一样。因为我们还没有进行反向传播计算，所以梯度的值全为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0. 0. 0. ... 0. 0. 0.]\n",
       " [0. 0. 0. ... 0. 0. 0.]\n",
       " [0. 0. 0. ... 0. 0. 0.]\n",
       " ...\n",
       " [0. 0. 0. ... 0. 0. 0.]\n",
       " [0. 0. 0. ... 0. 0. 0.]\n",
       " [0. 0. 0. ... 0. 0. 0.]]\n",
       "<NDArray 256x20 @cpu(0)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似地，我们可以访问其他层的参数，如输出层的偏差值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "<NDArray 10 @cpu(0)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[1].bias.data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们可以使用`collect_params`函数来获取`net`变量所有嵌套（例如通过`add`函数嵌套）的层所包含的所有参数。它返回的同样是一个由参数名称到参数实例的字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sequential6_ (\n",
       "  Parameter dense12_weight (shape=(256, 20), dtype=float32)\n",
       "  Parameter dense12_bias (shape=(256,), dtype=float32)\n",
       "  Parameter dense13_weight (shape=(10, 256), dtype=float32)\n",
       "  Parameter dense13_bias (shape=(10,), dtype=float32)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.collect_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数可以通过正则表达式来匹配参数名，从而筛选需要的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sequential6_ (\n",
       "  Parameter dense12_weight (shape=(256, 20), dtype=float32)\n",
       "  Parameter dense13_weight (shape=(10, 256), dtype=float32)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.collect_params('.*weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化模型参数\n",
    "\n",
    "我们在[“数值稳定性和模型初始化”](../chapter_deep-learning-basics/numerical-stability-and-init.ipynb)一节中描述了模型的默认初始化方法：权重参数元素为[-0.07, 0.07]之间均匀分布的随机数，偏差参数则全为0。但我们经常需要使用其他方法来初始化权重。MXNet的`init`模块里提供了多种预设的初始化方法。在下面的例子中，我们将权重参数初始化成均值为0、标准差为0.01的正态分布随机数，并依然将偏差参数清零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[ 1.3214819e-02  7.7018933e-03  2.7473064e-03 -8.0158552e-03\n",
       " -2.8782471e-03 -7.1328908e-04 -8.9433808e-03 -1.9323522e-02\n",
       " -7.6144733e-03 -3.8804002e-03 -9.2446133e-03 -9.0737930e-03\n",
       " -8.0585858e-05  1.2044553e-02  3.3023469e-03  1.1623081e-02\n",
       "  9.0154530e-03  4.0114806e-03 -7.4622743e-03 -2.2501945e-03]\n",
       "<NDArray 20 @cpu(0)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 非首次对模型初始化需要指定force_reinit为真\n",
    "net.initialize(init=init.Normal(sigma=0.01), force_reinit=True)\n",
    "net[0].weight.data()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面使用常数来初始化权重参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
       "<NDArray 20 @cpu(0)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.initialize(init=init.Constant(1), force_reinit=True)\n",
    "net[0].weight.data()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果只想对某个特定参数进行初始化，我们可以调用`Parameter`类的`initialize`函数，它与`Block`类提供的`initialize`函数的使用方法一致。下例中我们对隐藏层的权重使用Xavier随机初始化方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[ 0.09947211  0.08613716 -0.05103195  0.13325813 -0.11652803 -0.11376254\n",
       " -0.12281013  0.09271075  0.12890053  0.05080047 -0.11263967 -0.13778807\n",
       " -0.10588993 -0.02982824  0.10694438 -0.05353604 -0.07245647 -0.11701196\n",
       "  0.04893641 -0.05607144]\n",
       "<NDArray 20 @cpu(0)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.initialize(init=init.Xavier(), force_reinit=True)\n",
    "net[0].weight.data()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义初始化方法\n",
    "\n",
    "有时候我们需要的初始化方法并没有在`init`模块中提供。这时，可以实现一个`Initializer`类的子类，从而能够像使用其他初始化方法那样使用它。通常，我们只需要实现`_init_weight`这个函数，并将其传入的`NDArray`修改成初始化的结果。在下面的例子里，我们令权重有一半概率初始化为0，有另一半概率初始化为$[-10,-5]$和$[5,10]$两个区间里均匀分布的随机数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init  data_shape: dense12_weight       data_shape: (256, 20)\n",
      "Init  data_shape: dense13_weight       data_shape: (10, 256)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[-7.7221184  9.607502   0.        -7.956316   0.         6.5394897\n",
       " -6.473342  -5.7696404  8.834843   6.6189613  0.        -8.616304\n",
       " -0.         7.334059   0.        -0.         0.         8.496498\n",
       " -0.         0.       ]\n",
       "<NDArray 20 @cpu(0)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyInit(init.Initializer):\n",
    "    def _init_weight(self, name, data):\n",
    "        print('Init',  ' data_shape:' ,name, '      data_shape:' ,data.shape)\n",
    "        data[:] = nd.random.uniform(low=-10, high=10, shape=data.shape)\n",
    "        data *= data.abs() >= 5  #绝对值小于等于5，就置0\n",
    "\n",
    "net.initialize(MyInit(), force_reinit=True)\n",
    "net[0].weight.data()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，我们还可以通过`Parameter`类的`set_data`函数来直接改写模型参数。例如，在下例中我们将隐藏层参数在现有的基础上加1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[-6.7221184 10.607502   1.        -6.956316   1.         7.5394897\n",
       " -5.473342  -4.7696404  9.834843   7.6189613  1.        -7.6163044\n",
       "  1.         8.334059   1.         1.         1.         9.496498\n",
       "  1.         1.       ]\n",
       "<NDArray 20 @cpu(0)>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.set_data(net[0].weight.data()+1)\n",
    "#net[0].weight.set_data(nd.zeros((net[0].weight.data().shape)))\n",
    "net[0].weight.data()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 共享模型参数\n",
    "\n",
    "在有些情况下，我们希望在多个层之间共享模型参数。[“模型构造”](model-construction.ipynb)一节介绍了如何在`Block`类的`forward`函数里多次调用同一个层来计算。这里再介绍另外一种方法，它在构造层的时候指定使用特定的参数。如果不同层使用同一份参数，那么它们在前向计算和反向传播时都会共享相同的参数。在下面的例子里，我们让模型的第二隐藏层（`shared`变量）和第三隐藏层共享模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[1. 1. 1. 1. 1. 1. 1. 1.]\n",
       "<NDArray 8 @cpu(0)>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "shared = nn.Dense(8, activation='relu')\n",
    "net.add(nn.Dense(8, activation='relu'),\n",
    "        shared,\n",
    "        nn.Dense(8, activation='relu', params=shared.params),\n",
    "        nn.Dense(10))\n",
    "net.initialize()\n",
    "\n",
    "X = nd.random.uniform(shape=(2, 20))\n",
    "net(X)\n",
    "\n",
    "net[1].weight.data()[0] == net[2].weight.data()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在构造第三隐藏层时通过`params`来指定它使用第二隐藏层的参数。因为模型参数里包含了梯度，所以在反向传播计算时，第二隐藏层和第三隐藏层的梯度都会被累加在`shared.params.grad()`里。\n",
    "\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 有多种方法来访问、初始化和共享模型参数。\n",
    "* 可以自定义初始化方法。\n",
    "\n",
    "\n",
    "## 练习\n",
    "\n",
    "* 查阅有关`init`模块的MXNet文档，了解不同的参数初始化方法。\n",
    "* 尝试在`net.initialize()`后、`net(X)`前访问模型参数，观察模型参数的形状。\n",
    "* 构造一个含共享参数层的多层感知机并训练。在训练过程中，观察每一层的模型参数和梯度。\n",
    "\n",
    "\n",
    "\n",
    "## 扫码直达[讨论区](https://discuss.gluon.ai/t/topic/987)\n",
    "\n",
    "![](../img/qr_parameters.svg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
